{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "- Find information about the first dates of Non-Public Accounting jobs for each CPA Partners using a list of collected LinkedIn profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Since LinkedIn has a mechanism in place to prevent web scrapping that will restrict my account when suspicous activities are identified. I added some random pauses between each actions to mimic human actions and not getting blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to perform login\n",
    "def login_to_linkedin(login_info):\n",
    "    # Use selenium Chrome Driver to open the login page\n",
    "    browser.get('https://www.linkedin.com/uas/login')  \n",
    "    file = open(login_info)\n",
    "    # Read username and password saved in a text file\n",
    "    lines = file.readlines()\n",
    "    username = lines[0]\n",
    "    password = lines[1]\n",
    "    # Locate login input on the opened page and type in login info\n",
    "    login_info = browser.find_element_by_id('username')\n",
    "    login_info.send_keys(username)\n",
    "    time.sleep(random.randint(3,10)) # pause randomly for 5-10 seconds\n",
    "    login_info = browser.find_element_by_id('password')\n",
    "    login_info.send_keys(password)\n",
    "    time.sleep(random.randint(3,10))\n",
    "    login_info.submit()\n",
    "\n",
    "# Create function to extract information from a profile link\n",
    "def get_exp_info(profile_link):\n",
    "    browser.get(profile_link)\n",
    "    # Scroll the end of page first then scroll to the experience section\n",
    "    try:\n",
    "        browser.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "        time.sleep(random.randint(5,10))\n",
    "        browser.find_element_by_id('experience-section').location_once_scrolled_into_view\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "    # Then find the button to expand the experience section and show all job titles\n",
    "        time.sleep(random.randint(5,10)) \n",
    "        while True:\n",
    "            try:\n",
    "                browser.find_element_by_xpath('//li-icon[@type=\"chevron-down-icon\"][@class=\"pv-profile-section__toggle-detail-icon\"]').click()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break \n",
    "    \n",
    "    time.sleep(random.randint(5,10))\n",
    "    # Parsing source code\n",
    "    src = browser.page_source\n",
    "    soup = BeautifulSoup(src,'lxml')\n",
    "    # Extract experience section\n",
    "    exp_section = soup.find('section',{'id':'experience-section'})\n",
    "    if exp_section: # If there is an experience section then do this\n",
    "        exp_section = exp_section.find('ul')\n",
    "        li_tags = exp_section.find_all('li')\n",
    "        for i, li in enumerate(li_tags): #For each position in the list of positions do this \n",
    "            time.sleep(random.randint(5,10))\n",
    "            try: #Below is the main logic\n",
    "                a_tags = li.find('div').find('a')\n",
    "                if not li.find('div').find_all('a'): # Dealing with multiple positions within the same company\n",
    "                    comp_dirs.append('NA')\n",
    "                    companies.append('NA')\n",
    "                    if li.find_all('h3'):\n",
    "                        if li.find('h3').find_all('span'):\n",
    "                            titles.append(li.find('h3').find_all('span')[-1].get_text().strip())\n",
    "                        else:\n",
    "                            titles.append('NA')\n",
    "                    else:\n",
    "                        titles.append('NA')\n",
    "                    if li.find_all('h4'):\n",
    "                        if li.find('h4').find_all('span'):\n",
    "                            dates.append(li.find('h4').find_all('span')[-1].get_text().strip())\n",
    "                        else:\n",
    "                            dates.append('Jan 1900 – Jan 1900')\n",
    "                    else:\n",
    "                        dates.append('Jan 1900 – Jan 1900')\n",
    "                    links.append(profile_link)\n",
    "                    org_types.append('NA')\n",
    "                else: #Dealing with only 1 position per company\n",
    "                    p_tags = a_tags.find_all('p')\n",
    "                    if not p_tags:\n",
    "                        href = li.find('div').find('a',href=True)['href']\n",
    "                        comp_dirs.append(href)\n",
    "                        if a_tags.find_all('h3'):\n",
    "                            if a_tags.find('h3').find_all('span'):\n",
    "                                companies.append(a_tags.find('h3').find_all('span')[-1].get_text().strip())\n",
    "                            else:\n",
    "                                companies.append('NA')\n",
    "                        else:\n",
    "                            companies.append('NA')\n",
    "                        titles.append('NA')\n",
    "                        dates.append('Jan 1900 – Jan 1900')\n",
    "                        links.append(profile_link)\n",
    "                        time.sleep(random.randint(5,10))\n",
    "                        if href.startswith('/search/results') == False: #If the company has a LinkedIn profile then do this to get the type of company\n",
    "                            full_link = 'https://www.linkedin.com' + href\n",
    "                            browser.get(full_link)\n",
    "                            src = browser.page_source\n",
    "                            soup = BeautifulSoup(src,'lxml')\n",
    "                            if soup.find('div',{'class':'org-top-card-summary-info-list__info-item'}):\n",
    "                                org_info = soup.find('div',{'class':'org-top-card-summary-info-list__info-item'})\n",
    "                                org_types.append(org_info.get_text().strip())\n",
    "                            else:\n",
    "                                org_types.append('NA') #If there's no profile then append 'NA'\n",
    "                        else:\n",
    "                            org_types.append('NA')\n",
    "                    else:\n",
    "                        href = li.find('div').find('a', href=True)['href']\n",
    "                        comp_dirs.append(href)\n",
    "                        companies.append(a_tags.find_all('p')[-1].get_text().strip().split('\\n')[0])\n",
    "                        titles.append(a_tags.find('h3').get_text().strip())\n",
    "                        if a_tags.find('h4'):\n",
    "                            if a_tags.find('h4').find_all('span'):\n",
    "                                dates.append(a_tags.find('h4').find_all('span')[-1].get_text().strip())\n",
    "                            else:\n",
    "                                dates.append('Jan 1900 – Jan 1900')\n",
    "                        else:\n",
    "                            dates.append('Jan 1900 – Jan 1900')\n",
    "                        links.append(profile_link)\n",
    "                        time.sleep(random.randint(5,10))\n",
    "                        if href.startswith('/search/results') == False:\n",
    "                            full_link = 'https://www.linkedin.com' + href\n",
    "                            browser.get(full_link)\n",
    "                            src = browser.page_source\n",
    "                            soup = BeautifulSoup(src,'lxml')\n",
    "                            if soup.find('div',{'class':'org-top-card-summary-info-list__info-item'}):\n",
    "                                org_info = soup.find('div',{'class':'org-top-card-summary-info-list__info-item'})\n",
    "                                org_types.append(org_info.get_text().strip())\n",
    "                            else:\n",
    "                                org_types.append('NA')\n",
    "                        else:\n",
    "                            org_types.append('NA')\n",
    "            except: #Catching unexpected errors\n",
    "                comp_dirs.append('NA')\n",
    "                companies.append('NA')\n",
    "                titles.append('NA')\n",
    "                dates.append('Jan 1900 – Jan 1900')    \n",
    "                org_types.append('NA')\n",
    "                links.append(profile_link)\n",
    "    else: #If no experience section then do this\n",
    "        print('no experience section')\n",
    "        comp_dirs.append('NA')\n",
    "        companies.append('NA')\n",
    "        titles.append('NA')\n",
    "        dates.append('Jan 1900 – Jan 1900')    \n",
    "        org_types.append('NA')\n",
    "        links.append(profile_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set in np.arange(1,5,1): #Use more layers of random pausing in case the website has detected the existing patterns\n",
    "    try:   \n",
    "        for round in range(5): #We will do 5 rounds per set then take a random 5-10 minutes break\n",
    "            try:\n",
    "                # Compile a list of remaining profiles to be scraped\n",
    "                df_results = pd.read_csv('results.csv')\n",
    "                df_manual = pd.read_excel('manual results.xlsx')\n",
    "                df_partners = pd.read_excel('characteristics_for_partners_2016to2018_complete3733_corporate_experience_second_half.xlsx', sheet_name='Sheet1') #Original list of profiles\n",
    "                df_raw = pd.read_csv('df_raw.csv')\n",
    "                completed_profiles = np.append(df_results.linkedin_profiles.unique(),df_manual.linkedin_profiles.unique())\n",
    "                completed_profiles = np.append(completed_profiles, df_raw.linkedin_profiles.unique())\n",
    "                df_partners = df_partners[df_partners.weblink.notna()]\n",
    "                profiles = df_partners[~df_partners.weblink.isin(completed_profiles)].weblink[:10].values #Scrape 10 profiles per round\n",
    "                # Create empty lists to contain scrapped data\n",
    "                comp_dirs = []\n",
    "                companies = []\n",
    "                titles = []\n",
    "                dates = []\n",
    "                links = []\n",
    "                org_types = []\n",
    "                try_again_profiles = []\n",
    "                # login to LinkedIn\n",
    "                browser = webdriver.Chrome('chromedriver_win32/chromedriver')\n",
    "                login_to_linkedin('login info.txt')\n",
    "                time.sleep(random.randint(10,15))\n",
    "                # Start scraping\n",
    "                for n, p in enumerate(profiles):\n",
    "                    try:\n",
    "                        print('Start scraping set {} round {} profile {}'.format(set,round,p))\n",
    "                        get_exp_info(p)\n",
    "                        time.sleep(random.randint(3,5))\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(\"Can't scrape profile: {}\".format(p))\n",
    "                        try_again_profiles.append(p)\n",
    "                        try_again_text = open('try_again.txt','w') #Copy faulty profiles to a text file for further review\n",
    "                        for p in try_again_profiles:\n",
    "                            try_again_text.write(p + '\\n')\n",
    "                        try_again_text.close()\n",
    "                        break\n",
    "                # Save to data table\n",
    "                df = pd.DataFrame(list(zip(links,comp_dirs,companies,org_types,titles,dates)),columns=['linkedin_profiles','comp_dirs','companies','org_types','titles','dates'])\n",
    "                # Append to current table and print number of profiles remaining\n",
    "                df_raw = df_raw.append(df, ignore_index=True)\n",
    "                df_raw.to_csv('df_raw.csv', index=False)\n",
    "                profiles_remaining = df_partners.weblink.nunique() - df_results.linkedin_profiles.nunique() - df_manual.linkedin_profiles.nunique() - df_raw.linkedin_profiles.nunique()\n",
    "                print(profiles_remaining,'profiles remaining')\n",
    "                browser.close() #Close browser after each round\n",
    "                pause_period = random.randint(5,10)*60 #random 5-10 minutes break after each round\n",
    "                time.sleep(pause_period)\n",
    "            except Exception as e:       \n",
    "                print(e)\n",
    "                break #longer random pause can be placed here but since the current schedule appears to work well so there's no need for it\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "523b5db487c0a3af7702e50d2b1c86c4c50b41a6036bdc4af1442bf5ea6df93a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
